{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8HmuRDkwlJX",
        "outputId": "3c2f1046-d1ff-45f1-f50b-50c91ccf77cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchattacks in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.18.0+cu121)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.4)\n",
            "Requirement already satisfied: requests~=2.25.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.25.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->torchattacks) (12.5.40)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchattacks\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torchattacks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "classes = [str(i) for i in range(10)]\n"
      ],
      "metadata": {
        "id": "mvs-nszBoxYO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "batch_size = 4  # Define the batch size\n",
        "\n",
        "# Define the figure size\n",
        "plt.figure(figsize=(3 * batch_size, 4))\n",
        "\n",
        "# Show images and print labels for the first `batch_size` images\n",
        "for i in range(batch_size):\n",
        "    plt.subplot(1, batch_size, i + 1)  # 1 row, `batch_size` columns, i+1 denotes the position of the subplot\n",
        "    imshow(images[i])\n",
        "    plt.title(f\"{classes[labels[i]]}\")\n",
        "    plt.axis('off')  # Turn off axis\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cf_VqQ18o3DL",
        "outputId": "79211e6a-fc15-4ff2-f492-ba822fc6ec23"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-10602fa1c99a>:17: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
            "  plt.subplot(1, batch_size, i + 1)  # 1 row, `batch_size` columns, i+1 denotes the position of the subplot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n"
      ],
      "metadata": {
        "id": "5g8ykLwyo50U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "-FQkQzsLo8pt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 12\n",
        "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader: # get the inputs; images.to(device), labels.to(device) is a list of [images, labels]\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct_val / total_val\n",
        "\n",
        "    # Print training and validation statistics\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, '\n",
        "          f'Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "pje_Z65no-6o",
        "outputId": "590824c7-8dc5-4d5f-9899-58fc6209e5cc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-7f2597806da3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'mnist_model.pth')\n"
      ],
      "metadata": {
        "id": "qeu4C8wUpBVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "plt.figure(figsize=(3, 3))  # 3 columns and 4 rows\n",
        "\n",
        "for i in range(min(4, len(labels))):\n",
        "    plt.subplot(2, 2, i + 1)  # 2 rows, 2 columns, i+1 denotes the position of the subplot\n",
        "    imshow(images[i])\n",
        "    plt.title(f\"Label: {classes[labels[i]]}\")\n",
        "    plt.axis('off')  # Turn off axis\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "model.load_state_dict(torch.load('mnist_model.pth'))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')\n"
      ],
      "metadata": {
        "id": "HGPpk2FnpDmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the FGSM Attack\n",
        "attack_fgsm = torchattacks.FGSM(model, eps=8/255)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_fgsm(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TwLo_bk4pOiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the FFGSM Attack\n",
        "attack_ffgsm = torchattacks.FFGSM(model, eps=8/255)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_ffgsm(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xr0fVw-Hy7X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the RFGSM Attack\n",
        "attack_rfgsm = torchattacks.RFGSM(model, eps=8/255)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_rfgsm(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iRHcam31zJZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the EOTPGD Attack\n",
        "attack_eotpgd = torchattacks.EOTPGD(model, eps=8/255)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_eotpgd(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UdaXRmMt1Eql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the CW Attack\n",
        "attack_cw = torchattacks.CW(model)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_cw(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ijhn9pPm1fTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchattacks\n",
        "\n",
        "# Initialize the PGDL2 Attack\n",
        "attack_pgdl2 = torchattacks.PGDL2(model)\n",
        "\n",
        "# Initialize variables to count successful attacks\n",
        "successful_attacks = 0\n",
        "total_samples = 0\n",
        "\n",
        "# To store examples of successful attacks\n",
        "examples = []\n",
        "\n",
        "# Number of batches to process (subset of the test dataset)\n",
        "num_batches_to_process = 10  # Adjust this number based on your performance needs\n",
        "\n",
        "# Enable gradient computation\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    if batch_idx >= num_batches_to_process:\n",
        "        break\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Ensure images require gradients\n",
        "    images.requires_grad = True\n",
        "\n",
        "    adv_images = attack_pgdl2(images, labels)\n",
        "\n",
        "    # Predictions on original and adversarial images\n",
        "    original_predictions = model(images)\n",
        "    adversarial_predictions = model(adv_images)\n",
        "\n",
        "    original_pred_labels = torch.argmax(original_predictions, dim=1)\n",
        "    adversarial_pred_labels = torch.argmax(adversarial_predictions, dim=1)\n",
        "\n",
        "    # Compare predictions to determine if attack was successful\n",
        "    for i in range(len(labels)):\n",
        "        if original_pred_labels[i] == labels[i] and original_pred_labels[i] != adversarial_pred_labels[i]:\n",
        "            successful_attacks += 1\n",
        "            examples.append((images[i].cpu(), adv_images[i].cpu(), labels[i].cpu(), original_pred_labels[i].cpu(), adversarial_pred_labels[i].cpu()))\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "# Disable gradient computation\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = successful_attacks / total_samples\n",
        "\n",
        "\n",
        "# Print the success rate\n",
        "print(f'Success Rate: {success_rate:.4f}')\n",
        "\n",
        "# Visualization of some successful attack examples\n",
        "num_examples_to_show = min(5, len(examples))\n",
        "fig, axs = plt.subplots(num_examples_to_show, 2, figsize=(10, 5 * num_examples_to_show))\n",
        "for i in range(num_examples_to_show):\n",
        "    original_image, adv_image, true_label, orig_pred, adv_pred = examples[i]\n",
        "\n",
        "    # Original image\n",
        "    axs[i, 0].imshow(original_image.squeeze(), cmap='gray')\n",
        "    axs[i, 0].set_title(f\"Original Image\\nTrue: {classes[true_label]}\\nPred: {classes[orig_pred]}\")\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "    # Adversarial image\n",
        "    axs[i, 1].imshow(adv_image.squeeze(), cmap='gray')\n",
        "    axs[i, 1].set_title(f\"Adversarial Image\\nTrue: {classes[true_label]}\\nAdv Pred: {classes[adv_pred]}\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vhgr52oP3Fk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}